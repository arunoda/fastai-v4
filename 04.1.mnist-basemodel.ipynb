{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST - BaseModel\n",
    "\n",
    "Here we are trying to learn basics of deep learning with image classification as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some dataset\n",
    "\n",
    "For first, we need a dataset. For that, we are going to use the `MNIST` dataset. (Acutally we are using a subset of that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *\n",
    "from utils import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/storage/data/mnist_sample')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_path = untar_data(URLs.MNIST_SAMPLE)\n",
    "images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/storage/data/mnist_sample/labels.csv'),Path('/storage/data/mnist_sample/valid'),Path('/storage/data/mnist_sample/train')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/storage/data/mnist_sample/train/7'),Path('/storage/data/mnist_sample/train/3')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(images_path/\"train\").ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6131) [Path('/storage/data/mnist_sample/train/3/26208.png'),Path('/storage/data/mnist_sample/train/3/12882.png'),Path('/storage/data/mnist_sample/train/3/58219.png'),Path('/storage/data/mnist_sample/train/3/13242.png'),Path('/storage/data/mnist_sample/train/3/53225.png'),Path('/storage/data/mnist_sample/train/3/51947.png'),Path('/storage/data/mnist_sample/train/3/57974.png'),Path('/storage/data/mnist_sample/train/3/9812.png'),Path('/storage/data/mnist_sample/train/3/6658.png'),Path('/storage/data/mnist_sample/train/3/11285.png')...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(images_path/\"train\"/\"3\").ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_paths = (images_path/\"train\"/\"3\").ls().sorted()\n",
    "seven_paths = (images_path/\"train\"/\"7\").ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6265) [Path('/storage/data/mnist_sample/train/7/10002.png'),Path('/storage/data/mnist_sample/train/7/1001.png'),Path('/storage/data/mnist_sample/train/7/10014.png'),Path('/storage/data/mnist_sample/train/7/10019.png'),Path('/storage/data/mnist_sample/train/7/10039.png'),Path('/storage/data/mnist_sample/train/7/10046.png'),Path('/storage/data/mnist_sample/train/7/10050.png'),Path('/storage/data/mnist_sample/train/7/10063.png'),Path('/storage/data/mnist_sample/train/7/10077.png'),Path('/storage/data/mnist_sample/train/7/10086.png')...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxElEQVR4nGNgGNJApnHTBxGsMrJJW99+uv8vEkOCY1Xfjvc/nxVoGPxbiS7HH/nv37+jCgwMDD7/etDlZv/7t1uBlYGBgf/oZyVUOdZ9//50MDEwMDAwBP97gqaR/eG7cAiL5dg7a3Qr2dihjKp/O7F6hIGBgUH24Vc5nJLL//XjlKv/j9tQhy//0nDJMe3/t5MXl6Tlv9fGuOS4b+I2lCH53xkhnJLb/jmhuQHBNHbBqY2B6+S/DC5ckjyPXkjg1koiAAA9ID9NX4VZwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F256D8AD610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_seven = Image.open(seven_paths[10])\n",
    "im_seven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Image into Data\n",
    "\n",
    "To do something interesting, we need to convert these images into data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is the numpy array.\n",
    "array(im_seven).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is same thing but with pytorch. So, we will be using that from now on.\n",
    "tsr_seven = tensor(im_seven)\n",
    "tsr_seven.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This image is 2 dimention array (or a 2D matrix)\n",
    "## Let's play with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,   0,   0,   0,   0,   0,  15,  89, 254, 254, 254, 197,  32,   0,   0,   0,  76, 254, 140,   0,   0,   0,   0,   0,   0,   0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the 10th row\n",
    "tsr_seven[9,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,   0,   0,   0,   0,  98, 249,  32,   0,   0,   0,   0,   0,   0, 127, 244, 254, 235, 102,  18,   0,   0,   0,   0,   0,   0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This the 15th column\n",
    "tsr_seven[:, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,  15,  89, 254, 254, 254, 197,  32,   0,   0,   0,  76, 254, 140,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  15, 155, 254, 254, 187,  32,   5,   0,   0,   0,  15, 197, 243,  34,   0,   0,   0,   0,   0,   0,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is rows 10 and 11\n",
    "tsr_seven[9:11, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 89, 254, 254, 254, 197],\n",
       "        [254, 254, 187,  32,   5]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is rows 10 to 12 and colum 10 to 15\n",
    "tsr_seven[9:11, 9:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f256d7cf090>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABgAAABECAYAAACbO/V0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAADmUlEQVRYhe2XS0szSRSGnySdm+bSiZcomoWKRFARBFEQXIgbF25cijtduvGf+CP8CYIrVy5ERYKKSVSMBDQJaq5qkk7SXbOQuTAz6Y43vnHIgVpV1Xl436o+ddokhOA7w/yt2VuA/wRAMphv9oqZGk38fItagBagBfg/AIyK3T9CCIGqqmiahqIoaJqG1+ttuP7dCoQQVCoVisUi5+fnnJyc6K7XVRCPxzGZTFitVoQQKIpCtVolnU6Tz+eJRqMoisL8/HzDHCa9rmJjY0NIkkRXVxeqqpJIJCgWixwfH/P6+kq5XEbTNEqlUsP3QFdBNBrFarWSSqUQQpBMJqlUKthsNjweD5OTkzidTr0Ub542GiaTSfx9tLW1idnZWbG2tiZubm5EJpMRejl0FQghsFgsdHd3I8sy09PT+Hw+hoaG6Ovrw+fz4XA4dAUYXlOHw8HExATDw8Nsbm7S0dGB0+nEbDZjNpsxmRrabwyYm5ujs7OTxcVFBgYG8Pv92Gy2ppMbApaWlggEAiwvL9PW1tZUwncBFhYWcDqdWK3WDyUHg++AVl/UAvxsQK1W4/r6mvPz8+8B1Ot1Tk9P2d/f/1qAqqrk83nu7u7Y3d1lZ2dHd/27H31N08jlctzf33NwcEAikfg6gBCCarXK3t4e0WiUTCaDoihfB4A/vb+4uODl5QVVVb8GoKoqz8/PpFIpzs7OuLy8xGq14na7dfc1fciaplEoFHh8fCSRSPD09ITdbsflcunuM1QghPi9NWF7e5urqyuy2Sxer5eVlRV8Pt/nAPBmT6lUIhwOEw6HURQFWZYZHx+nr6/vc4BarUYikSAejxOJRMhmswSDQcbGxpiZmcHv9+vuNzwDTdNIp9Pc3t6Sy+VQFIWenh4CgQB+vx9Zlj+noF6vc3h4SCwWo1wuI8syq6urjI6O4vV6sdlsH1egaRqqqpJMJkkmk9RqNex2OyMjI/T39yNJEmazvgm6CrLZLA8PDxwdHRGLxahUKrrJ3q0gl8vx9PREKpWiUCjgdDpxu91IkoTFYvl847W1tUWxWCSTySDLMuvr64RCIUKhEC6Xy9AeQ0AkEuHl5YVarYbH42FkZIRQKER7e3vTzZguIJlMUq/XcblcBINBpqam6O3txWazfY1F/f39f5TowcFBvF4vDoej6cYXDFrHx8dHAW/1SJIk3G43Fovl37xvSGv1poZhVIs+9lPwl/j5FrUAvx7wG594tsvECaLNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize one of those tensors\n",
    "show_image(tsr_seven[:, 10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Here our idea is to create a model to detect 3 and 7 from our dataset. Before we are trying to do anything fancy, we need to create easy to create model. It should easy to create.\n",
    "\n",
    "Then we can compare our future fancy model with this baseline and see whether it's an improvement or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_threes = [tensor(Image.open(p)) for p in three_paths]\n",
    "lst_sevens = [tensor(Image.open(p)) for p in seven_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_threes[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the output is a list of tensors. \n",
    "\n",
    "**But in order to work with pytorch, we need to get convert them into a tensor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We also make those values from 0-1 range. That's the usual way we work with images.\n",
    "tsr_threes = torch.stack(lst_threes).float()/255\n",
    "tsr_sevens = torch.stack(lst_sevens).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6265, 28, 28])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsr_sevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f256b40c490>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(tsr_sevens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now we moved all of our images into a tensor. So, we can work on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Image\n",
    "\n",
    "In order to finish our baseline model, we will be get the mean of these stackes images and compare them with each of the image.\n",
    "\n",
    "So, first we need to get the mean image. Here's how we do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f256b3cc2d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsr_mean_threes = tsr_threes.mean(0)\n",
    "show_image(tsr_mean_threes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f256d7b4ed0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsr_mean_sevens = tsr_sevens.mean(0)\n",
    "show_image(tsr_mean_sevens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Number\n",
    "\n",
    "Now have stacked images of 3s, and 7s and we need to predict the number for a given image.\n",
    "Let's pick one of the 3s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f256b351490>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsr_three = tsr_threes[0]\n",
    "show_image(tsr_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "To do the prediction, we need to compare this image with the mean image. For that, we use a loss function. Simple pixel different won't work here due to positive and negative differences.\n",
    "\n",
    "For these loss functions, we can think of two methods\n",
    "\n",
    "1. Mean Absolute Difference (L1 norm)\n",
    "2. Root Mean Squared Error (L2 norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1074), tensor(0.1441))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. ABS - L1 norm\n",
    "\n",
    "loss_abs_for_three = (tsr_mean_threes - tsr_three).abs().mean()\n",
    "loss_abs_for_seven = (tsr_mean_sevens - tsr_three).abs().mean()\n",
    "loss_abs_for_three, loss_abs_for_seven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this image has lower loss for three rather seven. **So, we can predict this number as a three.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1912), tensor(0.2780))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 2. RMSE - L2 norm\n",
    "\n",
    "loss_rmse_for_three = ((tsr_mean_threes - tsr_three) ** 2).mean().sqrt()\n",
    "loss_rmse_for_seven = ((tsr_mean_sevens - tsr_three) ** 2).mean().sqrt()\n",
    "loss_rmse_for_three, loss_rmse_for_seven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with **RMSE** it's closer to three. But in this time, the difference between two losses are grater than previous. \n",
    "\n",
    "**So, RMSE might be a good choice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using built-in pytorch goodies.\n",
    "\n",
    "We can calculate above errors, very easily with pytorch's built-in functions.\n",
    "Let's try those for just for three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1074)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_loss_abs_for_three = F.l1_loss(tsr_mean_threes, tsr_three)\n",
    "pt_loss_abs_for_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1912)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_loss_rmse_for_three = F.mse_loss(tsr_mean_threes, tsr_three).sqrt()\n",
    "pt_loss_rmse_for_three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, they are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Some pyTorch Goodies\n",
    "\n",
    "Before we go further, we need to experiment with some pytorch goodies, we will be using next in this lesson.\n",
    "Those are:\n",
    "\n",
    "1. Broadcasting\n",
    "2. Tensor Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "We can do basic arithmatics even if the right hand side tensor's dimensions are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 25])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tensor([10, 20])\n",
    "b = tensor(5)\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15,  25],\n",
       "        [105, 205]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tensor([\n",
    "    [10, 20],\n",
    "    [100, 200]\n",
    "])\n",
    "b = tensor(5)\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 15,  21],\n",
       "        [105, 201]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tensor([\n",
    "    [10, 20],\n",
    "    [100, 200]\n",
    "])\n",
    "b = tensor([5, 1])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Dimensions\n",
    "\n",
    "It's very important to understand what tensor dimensions inorder to work with different operations into them.\n",
    "[This article](https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be) helps a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tensor(\n",
    "[\n",
    "    [\n",
    "        [10, 20, 30],\n",
    "        [100, 200, 300]\n",
    "    ],\n",
    "    \n",
    "    [\n",
    "        [10, 20, 30],\n",
    "        [100, 200, 300]\n",
    "    ]\n",
    "]\n",
    ").float()\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 60., 600.],\n",
       "        [ 60., 600.]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum((2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 120., 1200.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum((2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 20.,  40.,  60.],\n",
       "        [200., 400., 600.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum((0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 120., 1200.])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum((0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Accuracy\n",
    "\n",
    "Now we know how to work with the loss function. But we cannot use that loss function to asset the model over a set of images. What we need is accuracy.\n",
    "\n",
    "This is something used in common for classification related models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's load our validation images\n",
    "tsr_valid_threes = torch.stack([tensor(Image.open(im)).float()/255 for im in (images_path/\"valid\"/\"3\").ls()])\n",
    "tsr_valid_sevens = torch.stack([tensor(Image.open(im)).float()/255 for im in (images_path/\"valid\"/\"7\").ls()])\n",
    "tsr_valid_threes.shape, tsr_valid_sevens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(95.8416)\n"
     ]
    }
   ],
   "source": [
    "def find_threes_accuracy():\n",
    "    loss_threes = ((tsr_valid_threes - tsr_mean_threes)**2).mean((1, 2)).sqrt();\n",
    "    loss_sevens = ((tsr_valid_threes - tsr_mean_sevens)**2).mean((1, 2)).sqrt();\n",
    "    \n",
    "    is_three = loss_threes < loss_sevens\n",
    "    accuracy = (torch.sum(is_three == True).float() / len(is_three)) * 100.0\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "print(find_threes_accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(97.3735)\n"
     ]
    }
   ],
   "source": [
    "def find_sevens_accuracy():\n",
    "    loss_threes = ((tsr_valid_sevens - tsr_mean_threes)**2).mean((1, 2)).sqrt();\n",
    "    loss_sevens = ((tsr_valid_sevens - tsr_mean_sevens)**2).mean((1, 2)).sqrt();\n",
    "    \n",
    "    is_three = loss_threes < loss_sevens\n",
    "    accuracy = (torch.sum(is_three == False).float() / len(is_three)) * 100.0\n",
    "    \n",
    "    return accuracy\n",
    "    \n",
    "print(find_sevens_accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's generalize both of the above functions\n",
    "\n",
    "def find_accuracy_with_l1(tsr_input, expected_is_three):\n",
    "    loss_threes = (tsr_input - tsr_mean_threes).abs().mean((1, 2));\n",
    "    loss_sevens = (tsr_input - tsr_mean_sevens).abs().mean((1, 2));\n",
    "    \n",
    "    is_three = loss_threes < loss_sevens\n",
    "    accuracy = (torch.sum(is_three == expected_is_three).float() / len(is_three)) * 100.0\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def find_accuracy_with_l2(tsr_input, expected_is_three):\n",
    "    loss_threes = ((tsr_input - tsr_mean_threes)**2).mean((1, 2)).sqrt();\n",
    "    loss_sevens = ((tsr_input - tsr_mean_sevens)**2).mean((1, 2)).sqrt();\n",
    "    \n",
    "    is_three = loss_threes < loss_sevens\n",
    "    accuracy = (torch.sum(is_three == expected_is_three).float() / len(is_three)) * 100.0\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(91.6832), tensor(98.5409))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_threes = find_accuracy_with_l1(tsr_valid_threes, True)\n",
    "acc_sevens = find_accuracy_with_l1(tsr_valid_sevens, False)\n",
    "acc_threes, acc_sevens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(95.1120)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_accuracy = (acc_threes + acc_sevens)/2\n",
    "overall_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model is very good for comparing 3 and 7 even with the very simple basemodel.\n",
    "But 3 and 7 are very different letters. \n",
    "\n",
    "It's very interesting to compare these with 3 and 8 with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
